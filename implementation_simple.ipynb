{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "implementation_simple.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doscsy12/Data_sci_proj/blob/master/implementation_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a044cebd-be9b-4be9-8583-ed83c2415724"
      },
      "source": [
        "# CycleGAN in Pytorch"
      ],
      "id": "a044cebd-be9b-4be9-8583-ed83c2415724"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "964e80a3-34ef-4b5e-9d9a-68448ddce7ba"
      },
      "source": [
        "This youtube video from \"Two minutes paper\" provides a good summary of the method we are going to implement in this notebook:  \n",
        "\n",
        "[![AI Learns to Synthesize Pictures of Animals | Two Minute Papers #152](https://img.youtube.com/vi/D4C1dB9UheQ/0.jpg)](https://www.youtube.com/embed/D4C1dB9UheQ)\n",
        "\n",
        "I also recommend you to take a look at the project page by its authors: https://junyanz.github.io/CycleGAN/"
      ],
      "id": "964e80a3-34ef-4b5e-9d9a-68448ddce7ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9ab6a7f-944a-4360-b8dd-57b226e0f4fb"
      },
      "source": [
        "## Hyperparameters (on top for convinience)"
      ],
      "id": "b9ab6a7f-944a-4360-b8dd-57b226e0f4fb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe619df7-7113-4061-89af-f0be7e5cacb5"
      },
      "source": [
        "epoch = 0  # epoch to start training from\n",
        "n_epochs = 10  # number of epochs of training\n",
        "dataset_name = \"horse2zebra\"  # name of the dataset\n",
        "batch_size = 1  # size of the batches\n",
        "lr = 0.0002  # adam: learning rate\n",
        "b1 = 0.5  # adam: decay of first order momentum of gradient\n",
        "b2 = 0.999  # adam: decay of first order momentum of gradient\n",
        "decay_epoch = 1  # epoch from which to start lr decay\n",
        "n_cpu = 2  # number of cpu threads to use during batch generation\n",
        "img_height = 256  # size of image height\n",
        "img_width = 256  # size of image width\n",
        "channels = 3  # number of image channels\n",
        "sample_interval = 100  # interval between saving generator outputs\n",
        "checkpoint_interval = 1  # interval between saving model checkpoints\n",
        "n_residual_blocks = 9  # number of residual blocks in generator\n",
        "lambda_cyc = 10.0  # cycle loss weight\n",
        "lambda_id = 5.0  # identity loss weight"
      ],
      "id": "fe619df7-7113-4061-89af-f0be7e5cacb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "931c9672-6401-4753-afbc-480d4066a83b"
      },
      "source": [
        "## Imports"
      ],
      "id": "931c9672-6401-4753-afbc-480d4066a83b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65af2f54-5b13-4a57-9774-1c5858a96361"
      },
      "source": [
        "import datetime\n",
        "import glob\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch.autograd import Variable\n",
        "from torch.cuda.amp import autocast\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from tqdm import trange\n",
        "from tqdm.notebook import tqdm"
      ],
      "id": "65af2f54-5b13-4a57-9774-1c5858a96361",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynbKszferztY"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "id": "ynbKszferztY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a633ba8a-c30b-42d1-a10c-27f4c0fa605d"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"This notebook will run on GPU.\")\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    print(\"This notebook will run on CPU.\")\n",
        "    device = \"cpu\""
      ],
      "id": "a633ba8a-c30b-42d1-a10c-27f4c0fa605d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "595da249-a107-4795-a89d-875925bf256d"
      },
      "source": [
        "## Utilities"
      ],
      "id": "595da249-a107-4795-a89d-875925bf256d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6813cc49-3829-45da-851d-5b789dfb7e4e"
      },
      "source": [
        "# some helper functions to download the dataset\n",
        "# this code comes mainly from gluoncv.utils\n",
        "def download(url, path=None, overwrite=False) -> str:\n",
        "    \"\"\"Download an given URL.\n",
        "    Parameters\n",
        "    ----------\n",
        "    url : str\n",
        "        URL to download\n",
        "    path : str, optional\n",
        "        Destination path to store downloaded file. By default stores to the\n",
        "        current directory with same name as in url.\n",
        "    overwrite : bool, optional\n",
        "        Whether to overwrite destination file if already exists.\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The file path of the downloaded file.\n",
        "    \"\"\"\n",
        "    if path is None:\n",
        "        fname = url.split(\"/\")[-1]\n",
        "    else:\n",
        "        path = os.path.expanduser(path)\n",
        "        if os.path.isdir(path):\n",
        "            fname = os.path.join(path, url.split(\"/\")[-1])\n",
        "        else:\n",
        "            fname = path\n",
        "\n",
        "    if overwrite or not os.path.exists(fname):\n",
        "        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))\n",
        "        if not os.path.exists(dirname):\n",
        "            os.makedirs(dirname)\n",
        "\n",
        "        print(\"Downloading %s from %s...\" % (fname, url))\n",
        "        r = requests.get(url, stream=True)\n",
        "        if r.status_code != 200:\n",
        "            raise RuntimeError(\"Failed downloading url %s\" % url)\n",
        "        total_length = r.headers.get(\"content-length\")\n",
        "        with open(fname, \"wb\") as f:\n",
        "            if total_length is None:  # no content length header\n",
        "                for chunk in r.iter_content(chunk_size=1024):\n",
        "                    if chunk:  # filter out keep-alive new chunks\n",
        "                        f.write(chunk)\n",
        "            else:\n",
        "                total_length = int(total_length)\n",
        "                for chunk in tqdm(\n",
        "                    r.iter_content(chunk_size=1024),\n",
        "                    total=int(total_length / 1024.0 + 0.5),\n",
        "                    unit=\"KB\",\n",
        "                    unit_scale=False,\n",
        "                    dynamic_ncols=True,\n",
        "                ):\n",
        "                    f.write(chunk)\n",
        "    return fname\n",
        "\n",
        "\n",
        "def download_dataset(\n",
        "    dataset_name: str, data_path: str = \"data/\", overwrite: bool = False\n",
        ") -> None:\n",
        "    compatible_datasets = [\n",
        "        \"ae_photos\",\n",
        "        \"apple2orange\",\n",
        "        \"cezanne2photo\",\n",
        "        \"cityscapes\",\n",
        "        \"facades\",\n",
        "        \"grumpifycat\",\n",
        "        \"horse2zebra\",\n",
        "        \"iphone2dslr_flower\",\n",
        "        \"maps\",\n",
        "        \"mini\",\n",
        "        \"mini_colorization\",\n",
        "        \"mini_colorization\",\n",
        "        \"mini_pix2pix\",\n",
        "        \"monet2photo\",\n",
        "        \"summer2winter_yosemi\",\n",
        "        \"ukiyoe2photo\",\n",
        "        \"vangogh2photo\",\n",
        "    ]\n",
        "    if dataset_name not in compatible_datasets:\n",
        "        print(\"The dataset you chose is not compatible.\")\n",
        "        print(f\"Please select one among: {compatible_datasets}\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.mkdir(data_path)\n",
        "    download_url = f\"https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/{dataset_name}.zip\"\n",
        "    download_dir = os.path.join(data_path, \"downloads\")\n",
        "    if not os.path.exists(download_dir):\n",
        "        os.mkdir(download_dir)\n",
        "\n",
        "    filename = download(download_url, path=download_dir, overwrite=overwrite)\n",
        "\n",
        "    # Extract archive in target dir\n",
        "    with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(path=data_path)\n",
        "\n",
        "    # Re-organize dirs for more clarity\n",
        "    testdir = data_path + dataset_name + \"/\" + \"test\"\n",
        "    traindir = data_path + dataset_name + \"/\" + \"train\"\n",
        "    if not os.path.exists(testdir):\n",
        "        os.mkdir(testdir)\n",
        "    if not os.path.exists(traindir):\n",
        "        os.mkdir(traindir)\n",
        "    try:\n",
        "      os.rename(data_path + dataset_name + \"/\" + \"trainA\", traindir + \"/A\")\n",
        "      os.rename(data_path + dataset_name + \"/\" + \"trainB\", traindir + \"/B\")\n",
        "      os.rename(data_path + dataset_name + \"/\" + \"testA\", testdir + \"/A\")\n",
        "      os.rename(data_path + dataset_name + \"/\" + \"testB\", testdir + \"/B\")\n",
        "    except OSError:\n",
        "      pass\n",
        "\n",
        "    # Done\n",
        "    print(f\"Dataset downloaded and extracted in '{data_path}'.\")"
      ],
      "id": "6813cc49-3829-45da-851d-5b789dfb7e4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76f8b8c1-673a-486c-93ba-a5395a843cc3"
      },
      "source": [
        "download_dataset(dataset_name, overwrite=False)"
      ],
      "id": "76f8b8c1-673a-486c-93ba-a5395a843cc3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ab9a177-60ec-466c-a4a6-2afa0b1e41c1"
      },
      "source": [
        "## Dataset"
      ],
      "id": "5ab9a177-60ec-466c-a4a6-2afa0b1e41c1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "224a68aa-a356-42ed-8195-aafb28adbbea"
      },
      "source": [
        "class ImagesData(Dataset):\n",
        "    def __init__(self, root, data_augmentations=None, unaligned=False, dataset=\"train\"):\n",
        "        self.data_augmentations = data_augmentations\n",
        "        self.unaligned = unaligned\n",
        "\n",
        "        self.files_A = sorted(glob.glob(os.path.join(root, f\"{dataset}/A\") + \"/*.*\"))\n",
        "        self.files_B = sorted(glob.glob(os.path.join(root, f\"{dataset}/B\") + \"/*.*\"))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_A = Image.open(self.files_A[index % len(self.files_A)])\n",
        "\n",
        "        if self.unaligned:\n",
        "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
        "        else:\n",
        "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
        "\n",
        "        # Some images are graycale\n",
        "        # So we need to convert them to RGB\n",
        "        if image_A.mode != \"RGB\":\n",
        "          image_A = self.to_rgb(image_A)\n",
        "        if image_B.mode != \"RGB\":\n",
        "          image_B = self.to_rgb(image_B)\n",
        "\n",
        "        item_A = self.data_augmentations(image_A)\n",
        "        item_B = self.data_augmentations(image_B)\n",
        "        return {\"A\": item_A, \"B\": item_B}\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(len(self.files_A), len(self.files_B))\n",
        "\n",
        "    @staticmethod\n",
        "    def to_rgb(image):\n",
        "        rgb_image = Image.new(\"RGB\", image.size)\n",
        "        rgb_image.paste(image)\n",
        "        return rgb_image"
      ],
      "id": "224a68aa-a356-42ed-8195-aafb28adbbea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c9309c6-72a7-4aa3-9f08-7cda9ba2e6b2"
      },
      "source": [
        "# Normalization values are the ones from ImageNet\n",
        "# mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "\n",
        "# Training Data augmentations\n",
        "train_data_augmentations = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(int(img_height * 1.12), transforms.InterpolationMode.BICUBIC),\n",
        "        transforms.RandomCrop((img_height, img_width)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Test Data augmentations\n",
        "test_data_augmentations = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(int(img_height), transforms.InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")"
      ],
      "id": "3c9309c6-72a7-4aa3-9f08-7cda9ba2e6b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd7f19e0-7538-4c04-b916-3076ef147c8b"
      },
      "source": [
        "# Training Data Loader\n",
        "train_loader = DataLoader(\n",
        "    ImagesData(\n",
        "        f\"data/{dataset_name}\",\n",
        "        data_augmentations=train_data_augmentations,\n",
        "        unaligned=True,\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=n_cpu,\n",
        "    pin_memory=True,\n",
        ")\n",
        "# Test Data Loader\n",
        "test_loader = DataLoader(\n",
        "    ImagesData(\n",
        "        f\"data/{dataset_name}\",\n",
        "        data_augmentations=test_data_augmentations,\n",
        "        unaligned=True,\n",
        "        dataset=\"test\",\n",
        "    ),\n",
        "    batch_size=5,\n",
        "    shuffle=False,\n",
        "    num_workers=1,\n",
        ")"
      ],
      "id": "cd7f19e0-7538-4c04-b916-3076ef147c8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c30ae88-96af-476f-87fa-898b21c28d2c"
      },
      "source": [
        "## Models"
      ],
      "id": "6c30ae88-96af-476f-87fa-898b21c28d2c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75e7de85-365c-4ebb-8223-b9c8069ba722"
      },
      "source": [
        "### Generator\n",
        "\n",
        "From the paper, Appendix, 7.2. Network architectures: \n",
        "\n",
        "\"We  adopt  our  architectures from  Johnson  et  al. [23].  \n",
        "We  use 6 residual  blocks  for 128×128 training images,  \n",
        "and 9 residual blocks for 256×256 or higher-resolution  \n",
        "training images. Below, we followthe naming convention  \n",
        "used in the Johnson et al.’s Github repository.  \n",
        "Let c7s1-k denote a 7×7 Convolution-InstanceNorm-ReLU layer  \n",
        "with k filters and stride 1. dk denotes a 3×3 Convolution-InstanceNorm-ReLU   \n",
        "layer with k filters and stride 2. Reflection padding was  \n",
        "used to reduce artifacts. Rk denotes a residual block that  \n",
        "contains two 3×3 convolutional layers with the same number  \n",
        "of filters on both layer. uk denotes a 3×3 fractional-strided-Convolution-InstanceNorm-ReLU  \n",
        "layer with k filters and stride 1/2. \n",
        "\n",
        "The network with 6 residual blocks consists of:  \n",
        "c7s1-64,d128,d256,R256,R256,R256,R256,R256,R256,u128,u64,c7s1-3  \n",
        "The network with 9 residual blocks consists of:  \n",
        "c7s1-64,d128,d256,R256,R256,R256,R256,R256,R256,R256,R256,R256,u128u64,c7s1-3\"  "
      ],
      "id": "75e7de85-365c-4ebb-8223-b9c8069ba722"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b6b1cfa-fa66-4300-bc6b-1e0244f5527d"
      },
      "source": [
        "# Let's implement first the residual block.\n",
        "# We will re-use it many times\n",
        "# in the code.\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, 3),\n",
        "            nn.InstanceNorm2d(in_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, 3),\n",
        "            nn.InstanceNorm2d(in_features),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)"
      ],
      "id": "9b6b1cfa-fa66-4300-bc6b-1e0244f5527d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d63cbec1-2a64-4e70-b294-5110aa2ed234"
      },
      "source": [
        "# Now let's implement the generator according\n",
        "# to the paper.\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_shape: list, num_residual_blocks: int = 9):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        channels = input_shape[0]\n",
        "\n",
        "        # Initial convolution block\n",
        "        # c7s1-64\n",
        "        out_features = 64\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(channels),\n",
        "            nn.Conv2d(channels, out_features, 7),\n",
        "            nn.InstanceNorm2d(out_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        in_features = out_features\n",
        "\n",
        "        # Downsampling\n",
        "        # d128, d256\n",
        "        for _ in range(2):\n",
        "            out_features *= 2\n",
        "            model += [\n",
        "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            in_features = out_features\n",
        "\n",
        "        # Residual blocks\n",
        "        # R256 * <num_residual_blocks> (6 or 9 in the paper)\n",
        "        for _ in range(num_residual_blocks):\n",
        "            model += [ResidualBlock(out_features)]\n",
        "\n",
        "        # Upsampling\n",
        "        # u128, # u64\n",
        "        for _ in range(2):\n",
        "            out_features //= 2\n",
        "            model += [\n",
        "                nn.Upsample(scale_factor=2),\n",
        "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            in_features = out_features\n",
        "\n",
        "        # Output layer\n",
        "        # c7s1-3\n",
        "        model += [\n",
        "            nn.ReflectionPad2d(channels),\n",
        "            nn.Conv2d(out_features, channels, 7),\n",
        "            nn.Tanh(),\n",
        "        ]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "id": "d63cbec1-2a64-4e70-b294-5110aa2ed234",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f7b0a5b-f10a-4f35-8ae0-10b8980425bd"
      },
      "source": [
        "### Discriminator\n",
        "\n",
        "From the paper, Appendix, 7.2. Network architectures:\n",
        "\n",
        "For discriminator networks, we use 70×70 PatchGAN [22].  \n",
        "Let Ck denote a 4×4 Convolution-InstanceNorm-LeakyReLU  \n",
        "layer with k filters and stride 2. After the last layer,  \n",
        "we apply a convolution to produce a1-dimensional output.  \n",
        "We do not use InstanceNorm for the first C64 layer.  \n",
        "We use leaky ReLUs with a slope of 0.2.  \n",
        "\n",
        "The discriminator architecture is:  \n",
        "C64-C128-C256-C512  "
      ],
      "id": "7f7b0a5b-f10a-4f35-8ae0-10b8980425bd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "918da47f-7e7e-4246-b61b-0379c8dc93ae"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_shape: list):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        channels, height, width = input_shape\n",
        "\n",
        "        # Calculate output shape of image discriminator (PatchGAN)\n",
        "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
        "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(channels, 64, normalize=False),\n",
        "            # C64\n",
        "            *discriminator_block(64, 128),\n",
        "            # C128\n",
        "            *discriminator_block(128, 256),\n",
        "            # C256\n",
        "            *discriminator_block(256, 512),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            # C512\n",
        "            nn.Conv2d(512, 1, 4, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "id": "918da47f-7e7e-4246-b61b-0379c8dc93ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a5ded5d-d235-42fe-a57c-46b6157ba477"
      },
      "source": [
        "# Create sample and checkpoint directories\n",
        "os.makedirs(f\"images/{dataset_name}\", exist_ok=True)\n",
        "os.makedirs(f\"saved_models/{dataset_name}\", exist_ok=True)"
      ],
      "id": "8a5ded5d-d235-42fe-a57c-46b6157ba477",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c59cbf66-5840-4c1d-9734-85c21f26265a"
      },
      "source": [
        "# Losses\n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_cycle = torch.nn.L1Loss()\n",
        "criterion_identity = torch.nn.L1Loss()\n",
        "\n",
        "criterion_GAN.to(device)\n",
        "criterion_cycle.to(device)\n",
        "criterion_identity.to(device)"
      ],
      "id": "c59cbf66-5840-4c1d-9734-85c21f26265a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4e2f84c-aa97-42bd-b0c3-fb73f1343553"
      },
      "source": [
        "## Initializing our models"
      ],
      "id": "e4e2f84c-aa97-42bd-b0c3-fb73f1343553"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "724514f7-9478-454a-a03f-ff501351c5c9"
      },
      "source": [
        "input_shape = (channels, img_height, img_width)"
      ],
      "id": "724514f7-9478-454a-a03f-ff501351c5c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95460197-1c2a-49be-949c-c39efc14d778"
      },
      "source": [
        "# Initialize generator and discriminator\n",
        "G_AB = Generator(input_shape, n_residual_blocks)\n",
        "G_BA = Generator(input_shape, n_residual_blocks)\n",
        "D_A = Discriminator(input_shape)\n",
        "D_B = Discriminator(input_shape)"
      ],
      "id": "95460197-1c2a-49be-949c-c39efc14d778",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5ae057f-a56a-4154-9811-664db4b3ccd4"
      },
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        if hasattr(m, \"bias\") and m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)"
      ],
      "id": "f5ae057f-a56a-4154-9811-664db4b3ccd4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7934405f-2653-4a7e-a678-8fba47a3dead"
      },
      "source": [
        "# Initialize weights\n",
        "G_AB.apply(weights_init_normal)\n",
        "G_BA.apply(weights_init_normal)\n",
        "D_A.apply(weights_init_normal)\n",
        "D_B.apply(weights_init_normal)"
      ],
      "id": "7934405f-2653-4a7e-a678-8fba47a3dead",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05fb9c47-eade-4aed-aac6-e35b7152ce29"
      },
      "source": [
        "G_AB.to(device)\n",
        "G_BA.to(device)\n",
        "D_A.to(device)\n",
        "D_B.to(device)"
      ],
      "id": "05fb9c47-eade-4aed-aac6-e35b7152ce29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b78a2b8-9053-46c1-9e37-a3749580dcd5"
      },
      "source": [
        "G_AB.to(device)\n",
        "G_BA.to(device)\n",
        "D_A.to(device)\n",
        "D_B.to(device)"
      ],
      "id": "5b78a2b8-9053-46c1-9e37-a3749580dcd5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c821d056-d64c-4fa7-97ba-8961aba8858e"
      },
      "source": [
        "## Optimizers"
      ],
      "id": "c821d056-d64c-4fa7-97ba-8961aba8858e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fec6e9a3-2511-4bca-b6f3-4b9e01749b52"
      },
      "source": [
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(\n",
        "    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
        ")\n",
        "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))"
      ],
      "id": "fec6e9a3-2511-4bca-b6f3-4b9e01749b52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca0866b7-3990-42cb-b552-d606b631b768"
      },
      "source": [
        "## Learning rate scheduler"
      ],
      "id": "ca0866b7-3990-42cb-b552-d606b631b768"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e23a9e66-25fb-4e72-b237-055bc7b46017"
      },
      "source": [
        "class LambdaLR:\n",
        "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
        "        assert (\n",
        "            n_epochs - decay_start_epoch\n",
        "        ) > 0, \"Decay must start before the training session ends!\"\n",
        "        self.n_epochs = n_epochs\n",
        "        self.offset = offset\n",
        "        self.decay_start_epoch = decay_start_epoch\n",
        "\n",
        "    def step(self, epoch):\n",
        "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (\n",
        "            self.n_epochs - self.decay_start_epoch\n",
        "        )"
      ],
      "id": "e23a9e66-25fb-4e72-b237-055bc7b46017",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6596dd5d-f3ce-4188-853e-9219a4998b2c"
      },
      "source": [
        "# Learning rate update schedulers\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
        ")\n",
        "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
        ")\n",
        "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
        ")"
      ],
      "id": "6596dd5d-f3ce-4188-853e-9219a4998b2c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bae93407-c962-4629-9e77-892ba3382062"
      },
      "source": [
        "if device == \"cuda\":\n",
        "    Tensor = torch.cuda.FloatTensor\n",
        "else:\n",
        "    Tensor = torch.Tensor"
      ],
      "id": "bae93407-c962-4629-9e77-892ba3382062",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "409d0f2d-9bd0-4c24-8805-9a7bbc1d41d8"
      },
      "source": [
        "From the paper, 4. Implemention, Training details: \n",
        "\n",
        "[...] to  reduce  model  oscillation  [15],   \n",
        "we  follow Shrivastava  et  al.’s  strategy  [46]   \n",
        "and  update  the  discriminators using a history  \n",
        "of generated images rather than the ones produced  \n",
        "by the latest generators.  We keep an image buffer  \n",
        "that stores the 50 previously created images.  "
      ],
      "id": "409d0f2d-9bd0-4c24-8805-9a7bbc1d41d8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47f3f0a3-a2df-4d83-acec-0f2acf0af1d4"
      },
      "source": [
        "class ImageBuffer:\n",
        "    def __init__(self, max_size=50):\n",
        "        self.max_size = max_size\n",
        "        self.data = []\n",
        "\n",
        "    def push_and_pop(self, data):\n",
        "        to_return = []\n",
        "        for element in data.data:\n",
        "            element = torch.unsqueeze(element, 0)\n",
        "            if len(self.data) < self.max_size:\n",
        "                self.data.append(element)\n",
        "                to_return.append(element)\n",
        "            else:\n",
        "                if random.uniform(0, 1) > 0.5:\n",
        "                    i = random.randint(0, self.max_size - 1)\n",
        "                    to_return.append(self.data[i].clone())\n",
        "                    self.data[i] = element\n",
        "                else:\n",
        "                    to_return.append(element)\n",
        "        return Variable(torch.cat(to_return))"
      ],
      "id": "47f3f0a3-a2df-4d83-acec-0f2acf0af1d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bd7cb86-1993-44a7-bf4c-b5bc0256634b"
      },
      "source": [
        "# Buffers of previously generated samples\n",
        "fake_A_buffer = ImageBuffer()\n",
        "fake_B_buffer = ImageBuffer()"
      ],
      "id": "7bd7cb86-1993-44a7-bf4c-b5bc0256634b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fc61f21-fde6-4240-8b49-a51b4f0a7339"
      },
      "source": [
        "def sample_images(batches_done):\n",
        "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
        "    imgs = next(iter(test_loader))\n",
        "    G_AB.eval()\n",
        "    G_BA.eval()\n",
        "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
        "    fake_B = G_AB(real_A)\n",
        "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
        "    fake_A = G_BA(real_B)\n",
        "    # Arange images along x-axis\n",
        "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
        "    real_B = make_grid(real_B, nrow=5, normalize=True)\n",
        "    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n",
        "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
        "    # Arange images along y-axis\n",
        "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
        "    writer.add_image(\"Sample\", image_grid, batches_done)\n",
        "    save_image(image_grid, f\"images/{dataset_name}/{batches_done}.png\", normalize=False)"
      ],
      "id": "7fc61f21-fde6-4240-8b49-a51b4f0a7339",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d3c045c-2968-4a11-a4fe-e6f7446bd69b"
      },
      "source": [
        "## Training loop"
      ],
      "id": "9d3c045c-2968-4a11-a4fe-e6f7446bd69b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d118e1a-b39f-41cf-911e-b01697ab97f2"
      },
      "source": [
        "%tensorboard --logdir runs"
      ],
      "id": "8d118e1a-b39f-41cf-911e-b01697ab97f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2536a05a-1d98-4226-a602-b00e52be6891"
      },
      "source": [
        "if device == \"cuda\":\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "writer = SummaryWriter()\n",
        "\n",
        "writer.add_scalar(\"LearningRate/Generator\", lr_scheduler_G.get_last_lr()[0], epoch)\n",
        "writer.add_scalar(\n",
        "    \"LearningRate/DiscriminatorA\", lr_scheduler_D_A.get_last_lr()[0], epoch\n",
        ")\n",
        "writer.add_scalar(\n",
        "    \"LearningRate/DiscriminatorB\", lr_scheduler_D_B.get_last_lr()[0], epoch\n",
        ")\n",
        "\n",
        "\n",
        "for epoch in trange(epoch, n_epochs):\n",
        "    pbar = tqdm(total=len(train_loader))\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "        # Set model input\n",
        "        real_A = Variable(batch[\"A\"].type(Tensor))\n",
        "        real_B = Variable(batch[\"B\"].type(Tensor))\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(\n",
        "            Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False\n",
        "        )\n",
        "        fake = Variable(\n",
        "            Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False\n",
        "        )\n",
        "\n",
        "        # ------------------\n",
        "        #  Train Generators\n",
        "        # ------------------\n",
        "\n",
        "        G_AB.train()\n",
        "        G_BA.train()\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            with autocast():\n",
        "                # Identity loss\n",
        "                loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
        "                loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
        "        else:\n",
        "            # Identity loss\n",
        "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
        "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
        "\n",
        "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            with autocast():\n",
        "                # GAN loss\n",
        "                fake_B = G_AB(real_A)\n",
        "                loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
        "                fake_A = G_BA(real_B)\n",
        "                loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
        "        else:\n",
        "            # GAN loss\n",
        "            fake_B = G_AB(real_A)\n",
        "            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
        "            fake_A = G_BA(real_B)\n",
        "            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
        "\n",
        "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            with autocast():\n",
        "                # Cycle loss\n",
        "                recov_A = G_BA(fake_B)\n",
        "                loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
        "                recov_B = G_AB(fake_A)\n",
        "                loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
        "        else:\n",
        "            # Cycle loss\n",
        "            recov_A = G_BA(fake_B)\n",
        "            loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
        "            recov_B = G_AB(fake_A)\n",
        "            loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
        "\n",
        "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
        "\n",
        "        # Total loss\n",
        "        loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            scaler.scale(loss_G).backward()\n",
        "            scaler.step(optimizer_G)\n",
        "\n",
        "        else:\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "        # -----------------------\n",
        "        #  Train Discriminator A\n",
        "        # -----------------------\n",
        "\n",
        "        optimizer_D_A.zero_grad()\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            with autocast():\n",
        "                # Real loss\n",
        "                loss_real = criterion_GAN(D_A(real_A), valid)\n",
        "                # Fake loss (on batch of previously generated samples)\n",
        "                fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
        "                loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
        "        else:\n",
        "            # Real loss\n",
        "            loss_real = criterion_GAN(D_A(real_A), valid)\n",
        "            # Fake loss (on batch of previously generated samples)\n",
        "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
        "            loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
        "\n",
        "        # Total loss\n",
        "        loss_D_A = (loss_real + loss_fake) / 2\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            scaler.scale(loss_D_A).backward()\n",
        "            scaler.step(optimizer_D_A)\n",
        "        else:\n",
        "            loss_D_A.backward()\n",
        "            optimizer_D_A.step()\n",
        "\n",
        "        # -----------------------\n",
        "        #  Train Discriminator B\n",
        "        # -----------------------\n",
        "\n",
        "        optimizer_D_B.zero_grad()\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            with autocast():\n",
        "                # Real loss\n",
        "                loss_real = criterion_GAN(D_B(real_B), valid)\n",
        "                # Fake loss (on batch of previously generated samples)\n",
        "                fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
        "                loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
        "        else:\n",
        "            # Real loss\n",
        "            loss_real = criterion_GAN(D_B(real_B), valid)\n",
        "            # Fake loss (on batch of previously generated samples)\n",
        "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
        "            loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
        "\n",
        "        # Total loss\n",
        "        loss_D_B = (loss_real + loss_fake) / 2\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            scaler.scale(loss_D_B).backward()\n",
        "            scaler.step(optimizer_D_B)\n",
        "        else:\n",
        "            loss_D_B.backward()\n",
        "            optimizer_D_B.step()\n",
        "\n",
        "        loss_D = (loss_D_A + loss_D_B) / 2\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            scaler.update()\n",
        "\n",
        "        # --------------\n",
        "        #  Log Progress\n",
        "        # --------------\n",
        "\n",
        "        # Determine approximate time left\n",
        "        batches_done = epoch * len(train_loader) + i\n",
        "\n",
        "        writer.add_scalar(\"Loss/Generators\", loss_G, batches_done)\n",
        "        writer.add_scalar(\"Loss/Discriminators\", loss_D, batches_done)\n",
        "\n",
        "        # Save predictions every 'sample_interval'\n",
        "        if batches_done % sample_interval == 0:\n",
        "            sample_images(batches_done)\n",
        "        pbar.update(1)\n",
        "\n",
        "    # Update learning rates\n",
        "    lr_scheduler_G.step()\n",
        "    lr_scheduler_D_A.step()\n",
        "    lr_scheduler_D_B.step()\n",
        "    writer.add_scalar(\"LearningRate/Generator\", lr_scheduler_G.get_last_lr()[0], epoch)\n",
        "    writer.add_scalar(\n",
        "        \"LearningRate/DiscriminatorA\", lr_scheduler_D_A.get_last_lr()[0], epoch\n",
        "    )\n",
        "    writer.add_scalar(\n",
        "        \"LearningRate/DiscriminatorB\", lr_scheduler_D_B.get_last_lr()[0], epoch\n",
        "    )\n",
        "\n",
        "    if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
        "        # Save model checkpoints\n",
        "        torch.save(G_AB.state_dict(), f\"saved_models/{dataset_name}/G_AB_{epoch}.pth\")\n",
        "        torch.save(G_BA.state_dict(), f\"saved_models/{dataset_name}/G_BA_{epoch}.pth\")\n",
        "        torch.save(D_A.state_dict(), f\"saved_models/{dataset_name}/D_A_{epoch}.pth\")\n",
        "        torch.save(D_B.state_dict(), f\"saved_models/{dataset_name}/D_B_{epoch}.pth\")"
      ],
      "id": "2536a05a-1d98-4226-a602-b00e52be6891",
      "execution_count": null,
      "outputs": []
    }
  ]
}